{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "!pip install ipynb\r\n",
    "!pip install pandas\r\n",
    "!pip install scipy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: ipynb in c:\\users\\alxau\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\alxau\\anaconda3\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\alxau\\anaconda3\\lib\\site-packages (from pandas) (1.20.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\alxau\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\alxau\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alxau\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\alxau\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\alxau\\anaconda3\\lib\\site-packages (from scipy) (1.20.2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "# from .full.Gensim_Doc_Modelling import *\r\n",
    "# import ipynb.fs.defs.Topic_Analysis\r\n",
    "\r\n",
    "from gsdmm_master.gsdmm import MovieGroupProcess\r\n",
    "import pandas as pd\r\n",
    "import re\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
    "from collections import Counter\r\n",
    "\r\n",
    "\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import TweetTokenizer\r\n",
    "from matplotlib import colors as mcolors\r\n",
    "import scipy.sparse as sp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "def preprocess_dataset(dataset):\r\n",
    "    dataset = dataset.iloc[dataset.date_created_at.dropna().index]\r\n",
    "    dataset['date_index'] = pd.to_datetime(dataset.date_created_at, errors='coerce')\r\n",
    "    dataset = dataset.loc[dataset['date_index'].dropna().index]\r\n",
    "    \r\n",
    "    datasetusers = []\r\n",
    "\r\n",
    "    for mdate, df in dataset.groupby(pd.Grouper(key='date_index', freq='d')):\r\n",
    "        datasetusers.append(df.loc[df.user_id.drop_duplicates().index])\r\n",
    "\r\n",
    "    return pd.concat(datasetusers)\r\n",
    "\r\n",
    "def Map(func, lst):\r\n",
    "    return list(map(func, lst))\r\n",
    "\r\n",
    "def Filter(func, lst):\r\n",
    "    return list(filter(func, lst))\r\n",
    "\r\n",
    "def Enumerate(lst):\r\n",
    "    return list(enumerate(lst))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "def get_data_between(dataset, mindate, maxdate):\r\n",
    "    lowerbound = dataset[dataset.date_index > mindate]\r\n",
    "    return lowerbound[lowerbound.date_index < maxdate]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "import nltk\r\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alxau\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "tokenizer = TweetTokenizer()\r\n",
    "\r\n",
    "sw = stopwords.words('english')\r\n",
    "\r\n",
    "sw = sw + [',', '.', '!', '?', '’', '‘', '(', ')', '”', '“', '@', '/', '-']\r\n",
    "\r\n",
    "def transform_social_text(sentences, lower=True):\r\n",
    "    hashtags = re.compile(r\"^#\\S+|\\s#\\S+\")\r\n",
    "    mentions = re.compile(r\"^@\\S+|\\s@\\S+\")\r\n",
    "    urls = re.compile(r\"https?://\\S+\")\r\n",
    "\r\n",
    "    def process_text(text):\r\n",
    "        if lower:\r\n",
    "            text = text.lower()\r\n",
    "        text = hashtags.sub('', text)\r\n",
    "        text = mentions.sub('', text)\r\n",
    "        text = urls.sub('', text)\r\n",
    "        text = text.replace('\\n', ' .')\r\n",
    "\r\n",
    "        return text.strip()\r\n",
    "    \r\n",
    "    return list(map(process_text, sentences))\r\n",
    "\r\n",
    "def tokenize_tweet(tweet):\r\n",
    "    word_tokens = tokenizer.tokenize(tweet)\r\n",
    "    word_tokens = [w for w in word_tokens if not w in sw]\r\n",
    "    return word_tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "def traing_cluster_algo(data, K, iterations):\r\n",
    "    mgp = MovieGroupProcess(K=K, alpha=0.1, beta=0.1, n_iters=iterations)\r\n",
    "    mgp.fit(data, 1000)\r\n",
    "    return mgp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "def get_vocabularies(tokens):\r\n",
    "    cc = Counter()\r\n",
    "    for lst in tokens:\r\n",
    "        cc.update(lst)\r\n",
    "\r\n",
    "    vocab = dict([(i, v) for i, v in list(enumerate(map(lambda x :x[0], cc.most_common())))[:10000]])\r\n",
    "    \r\n",
    "    ## Add Unk Token\r\n",
    "    vocab[len(vocab)] = '$$unk$$'\r\n",
    "    \r\n",
    "    reverse_vocab = dict([(v , i) for i, v in vocab.items()])\r\n",
    "    return vocab, reverse_vocab, '$$unk$$'\r\n",
    "\r\n",
    "# Get Cluster numbers by date\r\n",
    "def get_cluster_timeseries(df, mdf):\r\n",
    "    timeseries = pd.merge(mdf['cluster'], df[['date_created_at']], how='left', left_index=True, right_index=True)\r\n",
    "    return timeseries\r\n",
    "    \r\n",
    "'''\r\n",
    "Returns a tuple of \r\n",
    "    (cluster_size,\r\n",
    "    cluster_id, \r\n",
    "    topwords,\r\n",
    "    most representative tweet, \r\n",
    "    avg engagement,\r\n",
    "    avg followers, \r\n",
    "    score)\r\n",
    "'''\r\n",
    "def addClusterMetrics(size2cluster, clusters, df, tweets_indexes, tweet_tokens):\r\n",
    "    mdf = pd.DataFrame([(mid, np.argmax(clusters.score(doc)), max(clusters.score(doc)), doc) for mid, doc in zip(tweets_indexes, tweet_tokens)])\r\n",
    "    mdf = mdf.set_index([0])\r\n",
    "    mdf.columns = ['cluster', 'score', 'text']\r\n",
    "    \r\n",
    "    # Cluster Id x Date\r\n",
    "    timeseries = get_cluster_timeseries(df, mdf)\r\n",
    "   \r\n",
    "    # Get Centroids (tweets)\r\n",
    "    result = pd.merge(mdf, df[['text', 'score_engagement', 'counts_followers']], how='left', left_index=True, right_index=True)\r\n",
    "    # Aggregate data by cluster (by avg)\r\n",
    "    aggregations = result.fillna(0).groupby('cluster').mean()\r\n",
    "    idx = result.groupby(['cluster'])['score'].transform(max) == mdf['score']\r\n",
    "    rr = result[idx].groupby(['cluster']).first()\r\n",
    "    centroids = pd.merge(aggregations, rr, how='left', on='cluster')['text_y']\r\n",
    "    \r\n",
    "    # Get cluster ids\r\n",
    "    cluster_numbers = mdf['cluster'].unique()\r\n",
    "    cluster_numbers.sort()\r\n",
    "    \r\n",
    "    return list(zip(map(lambda x:x[0], size2cluster), \r\n",
    "             cluster_numbers,\r\n",
    "             [\" \".join([w for w, s in lst]) for lst in map(lambda x:x[1], size2cluster)],\r\n",
    "             centroids,\r\n",
    "             aggregations['score_engagement'], \r\n",
    "             aggregations['counts_followers'],\r\n",
    "             (1 + aggregations['score_engagement']) * np.array(list(map(lambda x:x[0], size2cluster)))  )), timeseries\r\n",
    "\r\n",
    "\r\n",
    "'''Returns cluster aggregated data and the volume timeseries of clusters'''\r\n",
    "def extract_cluster(mgp, dataset, topk, tweets_indexes, tweet_tokens, order_pos=4):\r\n",
    "    \r\n",
    "    doc_count = mgp.cluster_doc_count\r\n",
    "    word_distribution = [\r\n",
    "        sorted(list(cluster.items()), key=lambda x: x[1], reverse=True)[:topk]\r\n",
    "        for cluster in mgp.cluster_word_distribution\r\n",
    "    ]\r\n",
    "    \r\n",
    "    \r\n",
    "    size2cluster = [(d, w) for d, w in zip(doc_count, word_distribution) if d > 0]\r\n",
    "    print(mgp.cluster_doc_count)\r\n",
    "    size2cluster, timeseries = addClusterMetrics(size2cluster, mgp, dataset, tweets_indexes, tweet_tokens)\r\n",
    "    size2cluster = sorted(size2cluster, key=lambda x: x[order_pos], reverse=True)\r\n",
    "    \r\n",
    "    return size2cluster, timeseries\r\n",
    "\r\n",
    "\r\n",
    "'''\r\n",
    "Compute the cluster algorithm on a dataset\r\n",
    "\r\n",
    "Parameters:\r\n",
    "    dataset: kpi6 dataset\r\n",
    "    minddate: from date\r\n",
    "    maxdate: to date\r\n",
    "    K: # cluster upper bound\r\n",
    "    iterations: of the clustering algorithm\r\n",
    "    topk: most representative words\r\n",
    "    order_pos: sort result by position of the tuple in the result\r\n",
    "    \r\n",
    "Returns:\r\n",
    "    cluster_aggregated_data (see #addClusterMetrics)\r\n",
    "    timeseries (see #get_cluster_timeseries)\r\n",
    "    cluster algorithm object\r\n",
    "'''\r\n",
    "def compute_clusters_between_dates(dataset, mindate, maxdate,\r\n",
    "                                   K, iterations, topk, order_pos = 4):\r\n",
    "    \r\n",
    "    selecteddata = get_data_between(dataset, mindate, maxdate)\r\n",
    "    tweets_indexes = selecteddata.text.dropna().index\r\n",
    "\r\n",
    "    processedtweets = transform_social_text(selecteddata.text.dropna())\r\n",
    "    processedtweets = [tokenize_tweet(tweet) for tweet in processedtweets]\r\n",
    "    processedtweets = list(filter(lambda x: len(x[1]) > 0, zip(tweets_indexes, processedtweets)))\r\n",
    "\r\n",
    "    tweets_indexes = list(map(lambda x:x[0], processedtweets))\r\n",
    "    tweet_tokens = list(map(lambda x:x[1], processedtweets))\r\n",
    "    \r\n",
    "    vocab, reverse_vocab, unk = get_vocabularies(tweet_tokens)\r\n",
    "    \r\n",
    "    clusters = traing_cluster_algo(tweet_tokens, K, iterations)\r\n",
    "    \r\n",
    "    aggregated_data, timeseries = extract_cluster(clusters, dataset, topk, tweets_indexes, tweet_tokens, order_pos)\r\n",
    "    \r\n",
    "    return aggregated_data, timeseries, clusters, ( vocab, reverse_vocab, unk), tweet_tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "def init_bottom(bottom, df):\r\n",
    "    for i in df.index:\r\n",
    "        if i not in bottom:\r\n",
    "            bottom[i] = 0\r\n",
    "\r\n",
    "def get_x_y(indexes, vals, bottom):\r\n",
    "    keys = sorted(list(bottom.keys()))\r\n",
    "    \r\n",
    "    vals = [vals[i] if i in indexes else 0 for i in keys]\r\n",
    "    return pd.DatetimeIndex(keys), vals\r\n",
    "\r\n",
    "def update_bottom(bottom, bottom_df):\r\n",
    "    for ix in bottom_df.index:\r\n",
    "        bottom[ix] += bottom_df.loc[ix]['val']\r\n",
    "        \r\n",
    "\r\n",
    "def get_bottom_series(bottom):\r\n",
    "    keys = sorted(list(bottom.keys()))\r\n",
    "    return [bottom[k] for k in keys]\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def get_timeseries(series):\r\n",
    "    %matplotlib inline\r\n",
    "    seriesc = series.copy()\r\n",
    "    seriesc['val'] = 1\r\n",
    "\r\n",
    "    seriesc['date_index'] =  pd.to_datetime(seriesc.date_created_at,  errors='coerce')\r\n",
    "\r\n",
    "    ts = seriesc.groupby(['cluster', pd.Grouper(key='date_index', freq='d')]).count()\r\n",
    "    multindex = ts.index\r\n",
    "    clusters = multindex.get_level_values(0).unique()\r\n",
    "    return [ts.loc[i] for i in clusters]\r\n",
    "\r\n",
    "\r\n",
    "def print_topic_chart(series, clusters):\r\n",
    "    clusters_sorted_by_num = sorted(clusters, key=lambda x:x[1])\r\n",
    "    topwords = list(map(lambda x:\" \".join(x[2].split(' ')[:7]), clusters_sorted_by_num))\r\n",
    "    \r\n",
    "    # get index\r\n",
    "    indexes = np.argsort(list(map(lambda x:x[0], clusters_sorted_by_num)))[::-1]\r\n",
    "    \r\n",
    "    \r\n",
    "    colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\r\n",
    "\r\n",
    "    # Sort colors by hue, saturation, value and name.\r\n",
    "    by_hsv = sorted((tuple(mcolors.rgb_to_hsv(mcolors.to_rgba(color)[:3])), name)\r\n",
    "                    for name, color in colors.items())\r\n",
    "    sorted_names = [name for hsv, name in by_hsv]\r\n",
    "    np.random.shuffle(sorted_names)\r\n",
    "\r\n",
    "    ss=get_timeseries(series)\r\n",
    "    \r\n",
    "    fig = plt.figure(figsize=(10, 8), constrained_layout=True)\r\n",
    "    axs = fig.subplots(1,1)\r\n",
    "    bottom = {}\r\n",
    "    \r\n",
    "    # get all indexes\r\n",
    "    \r\n",
    "    for i in range(len(ss)):\r\n",
    "        init_bottom(bottom, ss[i])\r\n",
    "        \r\n",
    "    for i in range(len(ss)):\r\n",
    "        plt.sca(axs)\r\n",
    "        plt.xticks(rotation=45)\r\n",
    "\r\n",
    "        axs.set_title('Volumes')\r\n",
    "        axs.grid(True)\r\n",
    "        \r\n",
    "        if i != 0:\r\n",
    "            s_indexes = ss[indexes[i]].index\r\n",
    "            \r\n",
    "            bottom_df = ss[indexes[i-1]]\r\n",
    "            update_bottom(bottom, bottom_df)\r\n",
    "            bottom_series = get_bottom_series(bottom)\r\n",
    "            X, y = get_x_y(ss[indexes[i]].index, ss[indexes[i]]['val'], bottom)\r\n",
    "            \r\n",
    "            axs.bar(X, y, bottom=bottom_series, color=sorted_names[indexes[i]])\r\n",
    "        else:\r\n",
    "            X, y = get_x_y(ss[indexes[i]].index, ss[indexes[i]]['val'], bottom)\r\n",
    "            axs.bar(X, y, color=sorted_names[indexes[i]])\r\n",
    "\r\n",
    "    axs.legend(np.array(sorted_names)[indexes], labels=np.array(topwords)[indexes], loc =\"upper left\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "nutella = pd.read_csv('data/nutella.csv') \r\n",
    "nutellanospam = preprocess_dataset(nutella)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\alxau\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,13,18,19,23,35,36,37,38,39,40,41,42,43,44,45,46,48,49,50,51,52) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "<ipython-input-148-b07506d9becb>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['date_index'] = pd.to_datetime(dataset.date_created_at, errors='coerce')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "print(nutellanospam)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         social                   id                              topic_id  \\\n",
      "393301  twitter  1178808882996666368  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
      "393300  twitter  1178810988868591616  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
      "393299  twitter  1178815313313488898  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
      "393298  twitter  1178816391421542400  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
      "393297  twitter  1178820636917059589  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
      "...         ...                  ...                                   ...   \n",
      "4       twitter  1362453782257422336  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
      "3       twitter  1363047761239769088  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
      "2       twitter  1363063651238731782  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
      "1       twitter  1363074677300690944  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
      "0       twitter  1363096473848659972  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
      "\n",
      "       rule_id is_comment                                               text  \\\n",
      "393301  9161.0      False  @thekryptikrose Thanks. I just squeezed my #Nu...   \n",
      "393300  9161.0      False          I don’t give a fuck #Nutella little bitch   \n",
      "393299  9161.0      False  Nutella with the mood lighting #crestedgecko #...   \n",
      "393298  9161.0      False  Today only! #Crepes #delicious #savory #yummy ...   \n",
      "393297  9161.0      False  Hazelnut Time!! Now serving the Hazelnut Nutel...   \n",
      "...        ...        ...                                                ...   \n",
      "4         9161      false  What is your unpopular opinion? Do you think B...   \n",
      "3         9161      false  Making up for Pancake Tuesday on Saturday with...   \n",
      "2         9161      false  Nutella is actually chocolate ki chutney😋 #fac...   \n",
      "1         9161      false  @VanessaFiji @orangulandtrust @griffjane @oran...   \n",
      "0         9161      false  @BlueCoyote62 @Benjaminknorr79 Luckily, @Nutel...   \n",
      "\n",
      "                                                   links  \\\n",
      "393301                                               NaN   \n",
      "393300                                               NaN   \n",
      "393299                           https://t.co/lRG2mhVDlc   \n",
      "393298                           https://t.co/jeSk4vC6Z9   \n",
      "393297                           https://t.co/OWMjRCBvV4   \n",
      "...                                                  ...   \n",
      "4                                https://t.co/nbVssJqOGr   \n",
      "3       https://t.co/M5BhTsfQZz, https://t.co/3m5ApdJNqW   \n",
      "2                                                    NaN   \n",
      "1                                                    NaN   \n",
      "0                                https://t.co/fYV80xa8I4   \n",
      "\n",
      "                                                 mentions  \\\n",
      "393301                                    @thekryptikrose   \n",
      "393300                                                NaN   \n",
      "393299                                                NaN   \n",
      "393298                                                NaN   \n",
      "393297                                                NaN   \n",
      "...                                                   ...   \n",
      "4                                                     NaN   \n",
      "3                                                     NaN   \n",
      "2                                          @nutellaglobal   \n",
      "1       @vanessafiji, @orangulandtrust, @griffjane, @o...   \n",
      "0       @bluecoyote62, @benjaminknorr79, @nutellagloba...   \n",
      "\n",
      "                                                 hashtags  \\\n",
      "393301                                           #nutella   \n",
      "393300                                           #nutella   \n",
      "393299                    #crestedgecko, #gecko, #nutella   \n",
      "393298  #crepes, #delicious, #savory, #yummy, #flavorf...   \n",
      "393297  #nutella, #hazelnut, #sweetobsessioncheesecake...   \n",
      "...                                                   ...   \n",
      "4       #unpopularopinion, #batman, #nutella, #pants, ...   \n",
      "3       #pancakes, #mardigras, #nutella, #raspberries,...   \n",
      "2                                        #facts, #nutella   \n",
      "1                                                     NaN   \n",
      "0       #orangutanfriendly, #choosesustainable, #palmo...   \n",
      "\n",
      "                                                  media  ...  \\\n",
      "393301                                              NaN  ...   \n",
      "393300                                              NaN  ...   \n",
      "393299  https://pbs.twimg.com/media/EFv-RcVX0AE7jU9.jpg  ...   \n",
      "393298                                              NaN  ...   \n",
      "393297                                              NaN  ...   \n",
      "...                                                 ...  ...   \n",
      "4                                                   NaN  ...   \n",
      "3       https://pbs.twimg.com/media/EuqEt0kXUAEO_kC.jpg  ...   \n",
      "2                                                   NaN  ...   \n",
      "1                                                   NaN  ...   \n",
      "0                                                   NaN  ...   \n",
      "\n",
      "       place_location_lat place_location_lon  place_lat   place_lon  \\\n",
      "393301                NaN                NaN        NaN         NaN   \n",
      "393300                NaN                NaN        NaN         NaN   \n",
      "393299                NaN                NaN        NaN         NaN   \n",
      "393298          34.086066        -117.668641  34.086066 -117.668641   \n",
      "393297                NaN                NaN        NaN         NaN   \n",
      "...                   ...                ...        ...         ...   \n",
      "4                     NaN                NaN        NaN         NaN   \n",
      "3                     NaN                NaN        NaN         NaN   \n",
      "2                     NaN                NaN        NaN         NaN   \n",
      "1                     NaN                NaN        NaN         NaN   \n",
      "0                     NaN                NaN        NaN         NaN   \n",
      "\n",
      "        user_place_country_code  score_spam  score_performance  \\\n",
      "393301                      NaN         NaN                NaN   \n",
      "393300                      NaN         NaN                NaN   \n",
      "393299                      NaN         NaN                NaN   \n",
      "393298                      NaN         NaN                NaN   \n",
      "393297                      NaN         NaN                NaN   \n",
      "...                         ...         ...                ...   \n",
      "4                           NaN         NaN                NaN   \n",
      "3                           NaN         NaN                NaN   \n",
      "2                           NaN         NaN                NaN   \n",
      "1                           NaN         NaN                NaN   \n",
      "0                           NaN         NaN                NaN   \n",
      "\n",
      "        user_score_rank score_rating                date_index  \n",
      "393301              NaN          NaN 2019-09-30 23:08:33+00:00  \n",
      "393300              NaN          NaN 2019-09-30 23:16:55+00:00  \n",
      "393299              NaN          NaN 2019-09-30 23:34:06+00:00  \n",
      "393298              NaN          NaN 2019-09-30 23:38:23+00:00  \n",
      "393297              NaN          NaN 2019-09-30 23:55:15+00:00  \n",
      "...                 ...          ...                       ...  \n",
      "4                   NaN          NaN 2021-02-18 17:27:49+00:00  \n",
      "3                   NaN          NaN 2021-02-20 08:48:05+00:00  \n",
      "2                   NaN          NaN 2021-02-20 09:51:13+00:00  \n",
      "1                   NaN          NaN 2021-02-20 10:35:02+00:00  \n",
      "0                   NaN          NaN 2021-02-20 12:01:39+00:00  \n",
      "\n",
      "[29621 rows x 63 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "# Drops inconsistent columns\r\n",
    "nutella_no_na = nutellanospam.dropna(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "source": [
    "nutella_no_na"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         social                   id                              topic_id  \\\n",
       "393301  twitter  1178808882996666368  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
       "393300  twitter  1178810988868591616  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
       "393299  twitter  1178815313313488898  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
       "393298  twitter  1178816391421542400  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
       "393297  twitter  1178820636917059589  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
       "...         ...                  ...                                   ...   \n",
       "4       twitter  1362453782257422336  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
       "3       twitter  1363047761239769088  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
       "2       twitter  1363063651238731782  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
       "1       twitter  1363074677300690944  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
       "0       twitter  1363096473848659972  d0f0c411-fca1-4412-8508-1348438d1db9   \n",
       "\n",
       "       rule_id is_comment                                               text  \\\n",
       "393301  9161.0      False  @thekryptikrose Thanks. I just squeezed my #Nu...   \n",
       "393300  9161.0      False          I don’t give a fuck #Nutella little bitch   \n",
       "393299  9161.0      False  Nutella with the mood lighting #crestedgecko #...   \n",
       "393298  9161.0      False  Today only! #Crepes #delicious #savory #yummy ...   \n",
       "393297  9161.0      False  Hazelnut Time!! Now serving the Hazelnut Nutel...   \n",
       "...        ...        ...                                                ...   \n",
       "4         9161      false  What is your unpopular opinion? Do you think B...   \n",
       "3         9161      false  Making up for Pancake Tuesday on Saturday with...   \n",
       "2         9161      false  Nutella is actually chocolate ki chutney😋 #fac...   \n",
       "1         9161      false  @VanessaFiji @orangulandtrust @griffjane @oran...   \n",
       "0         9161      false  @BlueCoyote62 @Benjaminknorr79 Luckily, @Nutel...   \n",
       "\n",
       "       lang_value           date_created_at  counts_following  \\\n",
       "393301         en  2019-09-30T23:08:33.000Z           12684.0   \n",
       "393300         en  2019-09-30T23:16:55.000Z               3.0   \n",
       "393299         en  2019-09-30T23:34:06.000Z             214.0   \n",
       "393298         en  2019-09-30T23:38:23.000Z              71.0   \n",
       "393297         en  2019-09-30T23:55:15.000Z              13.0   \n",
       "...           ...                       ...               ...   \n",
       "4              en  2021-02-18T17:27:49.000Z             739.0   \n",
       "3              en  2021-02-20T08:48:05.000Z              46.0   \n",
       "2              en  2021-02-20T09:51:13.000Z               5.0   \n",
       "1              en  2021-02-20T10:35:02.000Z            4950.0   \n",
       "0              en  2021-02-20T12:01:39.000Z           19533.0   \n",
       "\n",
       "        counts_followers  ...  counts_likes  score_engagement  \\\n",
       "393301           14863.0  ...           2.0          0.013366   \n",
       "393300             131.0  ...           0.0               0.0   \n",
       "393299             127.0  ...           1.0          0.969163   \n",
       "393298              20.0  ...           0.0               0.0   \n",
       "393297              36.0  ...           2.0          1.470588   \n",
       "...                  ...  ...           ...               ...   \n",
       "4                  208.0  ...           0.0               0.0   \n",
       "3                   30.0  ...           0.0               0.0   \n",
       "2                    0.0  ...           0.0               0.0   \n",
       "1                 4175.0  ...           0.0               0.0   \n",
       "0                17810.0  ...           0.0               0.0   \n",
       "\n",
       "                      user_id    user_username                    user_name  \\\n",
       "393301  1172501761430556672.0  misplacedcomma2              misplaced comma   \n",
       "393300           1053744668.0        Felony514               Flesh-N-Felony   \n",
       "393299             38630434.0    EmmyStormborn             EmeraldStormborn   \n",
       "393298           1285092918.0    tcbar_ontario    The Chocolate Bar Ontario   \n",
       "393297            203400288.0  Cheesecakewastd  Sweet Obsession Cheesecakes   \n",
       "...                       ...              ...                          ...   \n",
       "4                2973293529.0       hilebryson                  Bryson Hile   \n",
       "3                 596341763.0  SimonPastorello             Simon Pastorello   \n",
       "2       1355421219852484608.0       deivasri_v                   Deivasri V   \n",
       "1                 424144191.0            1BJDJ                        Ben 🙂   \n",
       "0                 225858328.0  orangulandtrust        Orangutan Land Trust🦧   \n",
       "\n",
       "                                     user_profile_picture user_gender_value  \\\n",
       "393301  https://pbs.twimg.com/profile_images/117439088...           unknown   \n",
       "393300  https://pbs.twimg.com/profile_images/125563236...           unknown   \n",
       "393299  https://pbs.twimg.com/profile_images/115176010...           unknown   \n",
       "393298  https://pbs.twimg.com/profile_images/116511029...           unknown   \n",
       "393297  https://pbs.twimg.com/profile_images/100961249...            female   \n",
       "...                                                   ...               ...   \n",
       "4       https://pbs.twimg.com/profile_images/136022794...              male   \n",
       "3       https://pbs.twimg.com/profile_images/114632529...              male   \n",
       "2       https://pbs.twimg.com/profile_images/136092300...           unknown   \n",
       "1       https://pbs.twimg.com/profile_images/127613935...              male   \n",
       "0       https://pbs.twimg.com/profile_images/683660955...           unknown   \n",
       "\n",
       "       user_lang_source user_verified                date_index  \n",
       "393301         provided           0.0 2019-09-30 23:08:33+00:00  \n",
       "393300         provided           0.0 2019-09-30 23:16:55+00:00  \n",
       "393299         provided           0.0 2019-09-30 23:34:06+00:00  \n",
       "393298         detected           0.0 2019-09-30 23:38:23+00:00  \n",
       "393297         provided           0.0 2019-09-30 23:55:15+00:00  \n",
       "...                 ...           ...                       ...  \n",
       "4              provided           0.0 2021-02-18 17:27:49+00:00  \n",
       "3              provided           0.0 2021-02-20 08:48:05+00:00  \n",
       "2              provided           0.0 2021-02-20 09:51:13+00:00  \n",
       "1              provided           0.0 2021-02-20 10:35:02+00:00  \n",
       "0              provided           0.0 2021-02-20 12:01:39+00:00  \n",
       "\n",
       "[29621 rows x 24 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>social</th>\n",
       "      <th>id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>rule_id</th>\n",
       "      <th>is_comment</th>\n",
       "      <th>text</th>\n",
       "      <th>lang_value</th>\n",
       "      <th>date_created_at</th>\n",
       "      <th>counts_following</th>\n",
       "      <th>counts_followers</th>\n",
       "      <th>...</th>\n",
       "      <th>counts_likes</th>\n",
       "      <th>score_engagement</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_username</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_profile_picture</th>\n",
       "      <th>user_gender_value</th>\n",
       "      <th>user_lang_source</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393301</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1178808882996666368</td>\n",
       "      <td>d0f0c411-fca1-4412-8508-1348438d1db9</td>\n",
       "      <td>9161.0</td>\n",
       "      <td>False</td>\n",
       "      <td>@thekryptikrose Thanks. I just squeezed my #Nu...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-30T23:08:33.000Z</td>\n",
       "      <td>12684.0</td>\n",
       "      <td>14863.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.013366</td>\n",
       "      <td>1172501761430556672.0</td>\n",
       "      <td>misplacedcomma2</td>\n",
       "      <td>misplaced comma</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/117439088...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>provided</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-30 23:08:33+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393300</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1178810988868591616</td>\n",
       "      <td>d0f0c411-fca1-4412-8508-1348438d1db9</td>\n",
       "      <td>9161.0</td>\n",
       "      <td>False</td>\n",
       "      <td>I don’t give a fuck #Nutella little bitch</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-30T23:16:55.000Z</td>\n",
       "      <td>3.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1053744668.0</td>\n",
       "      <td>Felony514</td>\n",
       "      <td>Flesh-N-Felony</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/125563236...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>provided</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-30 23:16:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393299</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1178815313313488898</td>\n",
       "      <td>d0f0c411-fca1-4412-8508-1348438d1db9</td>\n",
       "      <td>9161.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Nutella with the mood lighting #crestedgecko #...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-30T23:34:06.000Z</td>\n",
       "      <td>214.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.969163</td>\n",
       "      <td>38630434.0</td>\n",
       "      <td>EmmyStormborn</td>\n",
       "      <td>EmeraldStormborn</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/115176010...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>provided</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-30 23:34:06+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393298</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1178816391421542400</td>\n",
       "      <td>d0f0c411-fca1-4412-8508-1348438d1db9</td>\n",
       "      <td>9161.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Today only! #Crepes #delicious #savory #yummy ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-30T23:38:23.000Z</td>\n",
       "      <td>71.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1285092918.0</td>\n",
       "      <td>tcbar_ontario</td>\n",
       "      <td>The Chocolate Bar Ontario</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/116511029...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>detected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-30 23:38:23+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393297</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1178820636917059589</td>\n",
       "      <td>d0f0c411-fca1-4412-8508-1348438d1db9</td>\n",
       "      <td>9161.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Hazelnut Time!! Now serving the Hazelnut Nutel...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-30T23:55:15.000Z</td>\n",
       "      <td>13.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.470588</td>\n",
       "      <td>203400288.0</td>\n",
       "      <td>Cheesecakewastd</td>\n",
       "      <td>Sweet Obsession Cheesecakes</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/100961249...</td>\n",
       "      <td>female</td>\n",
       "      <td>provided</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-30 23:55:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1362453782257422336</td>\n",
       "      <td>d0f0c411-fca1-4412-8508-1348438d1db9</td>\n",
       "      <td>9161</td>\n",
       "      <td>false</td>\n",
       "      <td>What is your unpopular opinion? Do you think B...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-02-18T17:27:49.000Z</td>\n",
       "      <td>739.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2973293529.0</td>\n",
       "      <td>hilebryson</td>\n",
       "      <td>Bryson Hile</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/136022794...</td>\n",
       "      <td>male</td>\n",
       "      <td>provided</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-02-18 17:27:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1363047761239769088</td>\n",
       "      <td>d0f0c411-fca1-4412-8508-1348438d1db9</td>\n",
       "      <td>9161</td>\n",
       "      <td>false</td>\n",
       "      <td>Making up for Pancake Tuesday on Saturday with...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-02-20T08:48:05.000Z</td>\n",
       "      <td>46.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>596341763.0</td>\n",
       "      <td>SimonPastorello</td>\n",
       "      <td>Simon Pastorello</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/114632529...</td>\n",
       "      <td>male</td>\n",
       "      <td>provided</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-02-20 08:48:05+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1363063651238731782</td>\n",
       "      <td>d0f0c411-fca1-4412-8508-1348438d1db9</td>\n",
       "      <td>9161</td>\n",
       "      <td>false</td>\n",
       "      <td>Nutella is actually chocolate ki chutney😋 #fac...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-02-20T09:51:13.000Z</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1355421219852484608.0</td>\n",
       "      <td>deivasri_v</td>\n",
       "      <td>Deivasri V</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/136092300...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>provided</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-02-20 09:51:13+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1363074677300690944</td>\n",
       "      <td>d0f0c411-fca1-4412-8508-1348438d1db9</td>\n",
       "      <td>9161</td>\n",
       "      <td>false</td>\n",
       "      <td>@VanessaFiji @orangulandtrust @griffjane @oran...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-02-20T10:35:02.000Z</td>\n",
       "      <td>4950.0</td>\n",
       "      <td>4175.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>424144191.0</td>\n",
       "      <td>1BJDJ</td>\n",
       "      <td>Ben 🙂</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/127613935...</td>\n",
       "      <td>male</td>\n",
       "      <td>provided</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-02-20 10:35:02+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1363096473848659972</td>\n",
       "      <td>d0f0c411-fca1-4412-8508-1348438d1db9</td>\n",
       "      <td>9161</td>\n",
       "      <td>false</td>\n",
       "      <td>@BlueCoyote62 @Benjaminknorr79 Luckily, @Nutel...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-02-20T12:01:39.000Z</td>\n",
       "      <td>19533.0</td>\n",
       "      <td>17810.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225858328.0</td>\n",
       "      <td>orangulandtrust</td>\n",
       "      <td>Orangutan Land Trust🦧</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/683660955...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>provided</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-02-20 12:01:39+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29621 rows × 24 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "with open(file='data/authority_sets.txt', mode='r') as f:\r\n",
    "    category_list = []\r\n",
    "    category = f.readline()[:-1]\r\n",
    "    category_list.append(category)\r\n",
    "    \r\n",
    "    authority_sets = {}\r\n",
    "    authority_sets[category] = []\r\n",
    "    \r\n",
    "    lines = f.readlines()\r\n",
    "    for line in lines:\r\n",
    "        if(line[0] != '@'):\r\n",
    "            if(line != '\\n'):\r\n",
    "                category = line[:-1]\r\n",
    "                category_list.append(category)\r\n",
    "                authority_sets[category] = []\r\n",
    "        else:\r\n",
    "            if(line[-1:] == '\\n'):\r\n",
    "                authority_sets[category].append(line[1:-1])\r\n",
    "            else:\r\n",
    "                authority_sets[category].append(line[1:])\r\n",
    "\r\n",
    "for i in authority_sets:\r\n",
    "    print(i, authority_sets[i])\r\n",
    "\r\n",
    "categories = Enumerate(category_list)\r\n",
    "print(categories)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Politici ['matteosalvinimi', 'GiorgiaMeloni', 'EnricoLetta ', 'nzingaretti ', 'MonicaCirinna ', 'elenabonetti', 'matteorenzi', 'RossellaMuroni', 'luigidimaio', 'ale_dibattista', 'beppe_grillo', 'GiuseppeConteIT', 'NFratoianni', 'pbersani', 'pdnetwork', 'bobogiac', 'gasparripdl ', 'FratellidItalia', 'LegaSalvini', 'Capezzone', 'borghi_claudio', 'berlusconi', 'forza_italia', 'DarioNardella', 'carlaruocco1', 'gualtierieurope', 'BeaLorenzin', 'robersperanza', 'dariofrance', 'DaniloToninelli', 'BeppeSala', 'ellyesse', 'ElioLannutti', 'sbonaccini', 'marcocappato', 'PietroGrasso', 'ItaliaViva', 'zaiapresidente', 'TeresaBellanova', 'Azione_it', 'GuidoCrosetto', 'gparagone']\n",
      "Esperti di settore e giornalisti ['DarioBressanini', 'sabri_giannini', 'RudyBandiera', 'la_kuzzo', 'M_gabanelli', 'robertosaviano', 'petergomezblog', 'corradoformigli', 'IaconaRiccardo', 'marcotravaglio', 'AndreaScanzi', 'lucatelese', 'stanzaselvaggia', 'concitadeg', 'giucruciani', 'mariogiordano5', 'Tommasolabate', 'DAVIDPARENZO', 'myrtamerlino', 'alesallusti', 'NicolaPorro', 'BelpietroTweet', 'BrunoVespa', 'PaoloDebbio', 'GilettiMassimo', 'lucasofri', 'dariabig', 'divagatrice', 'RobiVil', 'distefanovalori']\n",
      "ONG&leaders ['Legambiente', 'Greenpeace_ITA', 'SaveChildrenIT', 'UNICEF_Italia', 'amnestyitalia', 'WWFitalia', 'StefanoCiafani', 'ap_legambiente', 'donabianchi1', 'ActionAidItalia', 'Fondoambiente', 'Italia_Nostra', 'FranFerrante', 'Kyoto_Club', 'robdellaseta', 'GiaSilvestrini', 'gonufrio']\n",
      "Sindacati e leader ['cgilnazionale', 'fiomnet', 'UILofficial', 'flaicgil', 'filleacgil', 'mauriziolandini', 'CislNazionale', 'FurlanAnnamaria', 'PpBombardieri', 'SusannaCamusso', 'fai_cisl', 'UGLConf']\n",
      "Influencer ['camillamendini', 'tessagelisio', 'LuciaCuffaro', 'MarcoBianchiOff', 'Link4Universe', 'Fedez', 'ChiaraFerragni', 'MarroneEmma', 'noemiofficial', 'chiarabiasi', 'VeronicaFerraro', 'andreadelogu', 'emastokholma', 'pif_iltestimone', 'federusso80', 'LucaBizzarri']\n",
      "programmi radio  e tv ['LaZanzaraR24', 'reportrai3', 'PiazzapulitaLA7', 'redazioneiene', 'Cartabiancarai3', 'mimandaRai3', 'Drittorovescio_', 'OttoemezzoTW', 'agorarai', 'Ariachetira', 'OmnibusLa7', 'welikeduel', 'QRepubblica', 'fuoridalcorotv', 'nonelarena', 'ChiVieneACena3']\n",
      "Associazioni e consorzi ['adocnazionale', 'MovConsumatori', 'adiconsum', 'consumatori', 'HelpConsumatori', 'Codacons', 'fedcons', 'massidona', 'ADUC1', 'Cittadinanzatti', 'Altroconsumo', 'CarloRienzi', 'pierani', 'fedcons', 'ConsumForum', 'Assoutenti', 'consumatori', 'Corepla_Riciclo', 'comieco', 'conai', '3filetti', 'MarevivoOnlus', 'plasticfreeit']\n",
      "testate mainstrem e portali verticali ['fattoquotidiano', 'ilpost', 'ilmessaggeroit', 'repubblica', 'Corriere', 'Agenzia_Ansa', 'VITAnonprofit', 'LaStampa', 'VanityFairIt', 'ilgiornale', 'Linkiesta', '_MicroMega_', 'giornalettismo', 'MilanoFinanza', 'greenMe_it', 'Gazzettino', 'Affaritaliani', 'GDS_it', 'GQitalia', 'ansa_ambiente', 'il_piccolo', 'altreconomia', 'Lantidiplomatic', 'FattoAlimentare', 'ItaliaaTavola', 'Ecodallecitta', 'alimentandonews', 'ilSalvagenteit', 'Greenreport_it', 'SecolodItalia1', 'Qualenergiait', 'EcoEcologia', 'Valori_it', 'rinnovabiliit', 'ilGiunco', 'lamescolanza', 'wireditalia', 'Agenzia_Italia', 'alfemminile_com', 'nostrofiglio', 'PianetaMamma', 'FattoreMamma']\n",
      "[(0, 'Politici'), (1, 'Esperti di settore e giornalisti'), (2, 'ONG&leaders'), (3, 'Sindacati e leader'), (4, 'Influencer'), (5, 'programmi radio  e tv'), (6, 'Associazioni e consorzi'), (7, 'testate mainstrem e portali verticali')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "interpreter": {
   "hash": "f446ed4898e137face024438143ad86cf21c8cccb3ccfa44cb119e4a51b03028"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}